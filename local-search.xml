<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Yield Curve Construction and Analysis</title>
    <link href="/2021/07/17/yield-curve/"/>
    <url>/2021/07/17/yield-curve/</url>
    
    <content type="html"><![CDATA[<p>A yield curve is a line that plots yields (interest rates) of bonds having equal credit quality but differing maturity dates. In this article, we survey a selection of the interpolation algorithms that are in use in financial markets for construction of curves such as forward curves and yield curves. The term structure of interest rates is defined as the relationship between the yield-to-maturity on a zero coupon bond and bond’s maturity.</p><span id="more"></span><h2 id="Yield-curve-mathematics"><a href="#Yield-curve-mathematics" class="headerlink" title="Yield curve mathematics"></a>Yield curve mathematics</h2><p>For a discount or zero coupon bond, the price of an instrument now at time $0$ which pays $1$ unit of currency at time $t$ is denoted $Z(0, t)$. The inverse of this amount is denoted $C(0, t)$ and called the capitalization factor. Note that $Z(0, t)$ is decreasing in $t$ for no arbitrage. Let the time $0$ continuous compounded risk free rate for maturity $t$ be denoted $r(t)$.</p><p>$$C(0, t) = \exp(r(t)t)$$</p><p>$$Z(0,t) = \exp(-r(t)t)$$</p><p>$$r(t) = - \frac{1}{t} \ln Z(0, t)$$</p><p>In normal markets, yield curves are upwardly sloping, with longer term interest rates being higher than short term. A yield curve which is downward sloping is called inverted. A yield curve with one or more turning points is called mixed. In a stable market with reasonably liquidity, one can observe a consistent mixed shape over long periods of time.</p><blockquote><p>While normal curves point to economic expansion, downward sloping (inverted) curves point to economic recession.</p></blockquote><h2 id="Interpolation-and-bootstrap"><a href="#Interpolation-and-bootstrap" class="headerlink" title="Interpolation and bootstrap"></a>Interpolation and bootstrap</h2><p>In finance, bootstrapping is a method for constructing a (zero-coupon) fixed-income yield curve from the prices of a set of coupon-bearing products, e.g. bonds and swaps. So far we assumed that bonds trade with sufficient liquidity and as a continuum i.e. a zero coupon bond exists for every redemption date $t$. In fact, such bonds rarely trade in the market, and we need to impute such a continuum via bootstrapping.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Use quadratic interpolation for curve fitting</span><br>r = interp1d(curve_dates, zero_rates, kind=<span class="hljs-string">&#x27;quadratic&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="bootstrap.png"></p><h2 id="Forward-rates"><a href="#Forward-rates" class="headerlink" title="Forward rates"></a>Forward rates</h2><p>The forward discount factor for the period from $t_1$ to $t_2$ at time $0$ satisfies the no arbitrage equation:</p><p>$$Z(0, t) Z(0; t_1, t_2) = Z(0, t_2)$$</p><p>The forward rate governing the period from $t_1$ to $t_2$, denoted $f(0; t_1, t_2)$, satisfies:</p><p>$$\exp(-f(0; t_1, t_2)(t_2 - t_1)) = Z(0; t_1, t_2)$$</p><p>Note that forward rates are positive and we have:</p><p>$$f(0; t_1, t_2) = -\frac{\ln Z(0, t_2) - \ln Z(0, t_1)}{t_2 - t_1} = - \frac{r_2 t_2 - r_1 t_1}{t_2 - t_1}$$</p><h2 id="Zero-curve-to-forward-curve-conversion"><a href="#Zero-curve-to-forward-curve-conversion" class="headerlink" title="Zero curve to forward curve conversion"></a>Zero curve to forward curve conversion</h2><p>Let instantaneous forward rate for a tenor of $t$ be denoted $f(t)$, we have:</p><p>$$f(t) = \frac{d}{dt} r(t)t = r(t) + r^{\prime}(t)t$$</p><p>Hence, the forward rates will lie above the yield curve when the yield curve is normal, and below the yield curve when it is inverted.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Use backward difference for numerical differentiation</span><br>f = <span class="hljs-keyword">lambda</span> t: r(t) + np.diff(r(t), prepend=r(t)[<span class="hljs-number">0</span>]) * t / dt<br></code></pre></td></tr></table></figure><h2 id="Forward-curve-to-zero-curve-conversion"><a href="#Forward-curve-to-zero-curve-conversion" class="headerlink" title="Forward curve to zero curve conversion"></a>Forward curve to zero curve conversion</h2><p>Given the forward rate function, we can find the risk free function by integrating:</p><p>$$r(t)t = \int_{0}^{t} f(s) ds = r_{i-1}t_{i-1} + \int_{t_{i-1}}^{t} f(s)ds, \quad t \in [t_{i-1}, t_i]$$</p><p>Note that the average of the instantaneous forward rate over any of our intervals $[t_{i-1}, t_i]$ is equal to the discrete forward rate for that interval.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Use dynamic programming for numerical integration</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(t)):<br>r[i] = (r[i-<span class="hljs-number">1</span>] * t[i-<span class="hljs-number">1</span>] + np.trapz([f[i-<span class="hljs-number">1</span>], f[i]], [t[i-<span class="hljs-number">1</span>], t[i]])) / t[i]<br></code></pre></td></tr></table></figure><h2 id="Bank-of-England-UK-yield-curve-data"><a href="#Bank-of-England-UK-yield-curve-data" class="headerlink" title="Bank of England UK yield curve data"></a>Bank of England UK yield curve data</h2><p>The government liability curve is based on yields on UK government bonds (gilts) and yields in the general collateral repo market. The nominal yield curves are derived from UK gilt prices and General Collateral (GC) repo rates. The real yield curves are derived from UK index-linked bond prices. Using the Fisher relationship, we are also able to estimate a term structure of inflation expectations for the United Kingdom. </p><p><img src="glc.png"></p>]]></content>
    
    
    <categories>
      
      <category>Quantitative Finance</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Quant</tag>
      
      <tag>Finance</tag>
      
      <tag>Capital market</tag>
      
      <tag>Bond</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Part IIA Engineering Tripos Revision Notes</title>
    <link href="/2021/07/16/part-iia/"/>
    <url>/2021/07/16/part-iia/</url>
    
    <content type="html"><![CDATA[<p>This is the revision notes for Part IIA of the Engineering Tripos at the University of Cambridge. The modules taken are part of Information and Computer Engineering, Electrical and Information Sciences, Instrumentation and Control.</p><span id="more"></span><p>Download the pdf version of the revision notes here:</p><ul><li><a href="Radio_Frequency_Electronics.pdf" download>3B1 Radio Frequency Electronics</a></li><li><a href="Semiconductor_Engineering.pdf" download>3B5 Semiconductor Engineering</a></li><li><a href="Signals_and_Systems.pdf" download>3F1 Signals and Systems</a></li><li><a href="Systems_and_Control.pdf" download>3F2 Systems and Control</a></li><li><a href="Statistical_Signal_Processing.pdf" download>3F3 Statistical Signal Processing</a></li><li><a href="Data_Transmission.pdf" download>3F4 Data Transmission</a></li><li><a href="Information_Theory_and_Coding.pdf" download>3F7 Information Theory and Coding</a></li><li><a href="Inference.pdf" download>3F8 Inference</a></li><li><a href="Modelling_Risk.pdf" download>3E3 Modelling Risk</a></li><li><a href="Mathematical_Methods.pdf" download>3M1 Mathematical Methods</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Cambridge</tag>
      
      <tag>Engineering</tag>
      
      <tag>Information</tag>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generating Pokemon with Deep Convolutional GANs</title>
    <link href="/2020/05/06/pokemon-generation/"/>
    <url>/2020/05/06/pokemon-generation/</url>
    
    <content type="html"><![CDATA[<p>Generative Adversarial Networks (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A generator learns to create images that look real, while a discriminator learns to tell real images apart from fakes. This project aims to use a deep convolutional GAN to generate Pokemons and classify their types. Visit the website at <a href="https://ziyizhu.me/pokemon-generator/">Pokemon Generator</a>.</p><span id="more"></span><p><img src="dcgan.gif"></p><h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><p>Code for this project can be found in <a href="https://github.com/ziyi-zhu/pokemon-generator">GitHub repository</a>.</p><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><p>Install Express in the <code>root</code> directory and save it in the dependencies list. For example:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install express --save<br></code></pre></td></tr></table></figure><p>To install Express temporarily and not add it to the dependencies list:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install express --no-save<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Node.js</tag>
      
      <tag>Javascript</tag>
      
      <tag>Python</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>Machine learning</tag>
      
      <tag>GANs</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Optical Bouncer - AR Game Made for Browser</title>
    <link href="/2020/03/30/ar-browser-game/"/>
    <url>/2020/03/30/ar-browser-game/</url>
    
    <content type="html"><![CDATA[<p>Augmented reality (AR) is an interactive experience of a real-world environment where the objects that reside in the real world are enhanced by computer-generated perceptual information. This simple game uses computer vision to achieve the illusion of AR and allow players to control the game by moving real objects. This game is also built with Matter.js which is a JavaScript 2D rigid body physics engine for the web. Visit the website at <a href="https://ziyizhu.me/optical-bouncer/">Optical Bouncer</a>.</p><span id="more"></span><p><img src="gameplay.gif"></p><h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><p>Code for this project can be found in <a href="https://github.com/ziyi-zhu/optical-bouncer">GitHub repository</a>.</p><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><p>Install Express and Matter.js in the <code>root</code> directory and save it in the dependencies list. For example:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install --save express matter-js<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Node.js</tag>
      
      <tag>Javascript</tag>
      
      <tag>Matter.js</tag>
      
      <tag>Computer vision</tag>
      
      <tag>AR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Virus Simulation with Quadtree Implementation in Javascript</title>
    <link href="/2020/03/24/virus-simulation/"/>
    <url>/2020/03/24/virus-simulation/</url>
    
    <content type="html"><![CDATA[<p>The outbreak of the respiratory virus in China has caused an unprecedented global response over the past few months. The spread of a virus can be affected by several factors such as the extent of social distancing and incubation periods of the virus. This project attempts to model the spread of such a virus while impelementing a quadtree data structure to efficiently store data of points on a two-dimensional space. Visit the website at <a href="https://ziyizhu.me/virus-simulator/">Virus Simulator</a>.</p><span id="more"></span><p><img src="spread.gif"></p><h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><p>Code for this project can be found in <a href="https://github.com/ziyi-zhu/virus-simulator">GitHub repository</a>.</p><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><p>Install Express in the <code>root</code> directory and save it in the dependencies list. For example:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install express --save<br></code></pre></td></tr></table></figure><p>To install Express temporarily and not add it to the dependencies list:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install express --no-save<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Node.js</tag>
      
      <tag>Javascript</tag>
      
      <tag>Processing</tag>
      
      <tag>Data analysis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Part IB Engineering Tripos Revision Notes</title>
    <link href="/2020/03/24/part-ib/"/>
    <url>/2020/03/24/part-ib/</url>
    
    <content type="html"><![CDATA[<p>This is the revision notes for Part IB of the Engineering Tripos at the University of Cambridge. The modules taken are part of Information and Computer Engineering, Electrical and Information Sciences, Instrumentation and Control.</p><span id="more"></span><p>Download the pdf version of the revision notes here:</p><ul><li><a href="linear-systems-and-control.pdf" download>Paper 6 Linear Systems and Control</a></li><li><a href="signal-and-data-analysis.pdf" download>Paper 6 Signal and Data Analysis</a></li><li><a href="communications.pdf" download>Paper 6 Communications</a></li><li><a href="linear-algebra.pdf" download>Paper 7 Linear Algebra</a></li><li><a href="probability.pdf" download>Paper 7 Probability</a></li><li><a href="vector-calculus.pdf" download>Paper 7 Vector Calculus</a></li><li><a href="engineer-in-business.pdf" download>Paper 8 Engineer in Business</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Notes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Cambridge</tag>
      
      <tag>Engineering</tag>
      
      <tag>Information</tag>
      
      <tag>Computer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Visualization for Image Processing Algorithms</title>
    <link href="/2020/03/17/image-processing-visualization/"/>
    <url>/2020/03/17/image-processing-visualization/</url>
    
    <content type="html"><![CDATA[<p>In image processing, a kernel, convolution matrix, or mask is a small matrix. It is used for blurring, sharpening, embossing, edge detection, and other useful algorithms for feature extraction and matching. This process is accomplished by doing a two dimensional convolution between a kernel and an image. This project aims to visualize the process of such algorithms. See the algorithms in action on <a href="https://ziyizhu.me/image-processing-visualizer/">Image Processing Visuailizer</a>.</p><span id="more"></span><p><img src="original.jpg"></p><h2 id="Image-processing-algorithms-for-feature-extraction"><a href="#Image-processing-algorithms-for-feature-extraction" class="headerlink" title="Image processing algorithms for feature extraction"></a>Image processing algorithms for feature extraction</h2><h3 id="1D-edge-detection"><a href="#1D-edge-detection" class="headerlink" title="1D edge detection"></a>1D edge detection</h3><p>A broad overview of 1D edge detection is to convolve the signal $I(x)$ with a <strong>Gaussian kernel</strong> $g_\sigma(x)$ and call the smoothed signal $s(x)$. Then compute the derivative $s^{‘}(x)$ and find its maxima and minima. Use thresholding on the magnitude of extrema to mark edges.</p><p>$$g_{\sigma}(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}}$$ </p><p>$$s(x) = g_{\sigma}(x) * I(x) = \int_{-\infty}^{\infty} g_{\sigma}(u)I(x-u)du = \int_{-\infty}^{\infty} g_{\sigma}(x-u)I(u)du$$</p><p><img src="edge_detection.png"></p><p>The differentiation is also performed by a 1D convolution. However, we can compute $s^{‘}(x)$ by convolving only once using the <strong>derivative theorem of convolution</strong>:</p><p>$$s^{‘}(x) = \frac{d}{dx} \left[ g_{\sigma}(x) * I(x) \right] = g_{\sigma}^{‘}(x) * I(x)$$</p><p><img src="edge_detection_simplified.png"></p><p>Looking for maxima and minima of $s^{‘}(x)$ is the same as looking for zero-crossings of $s^{‘’}(x)$. In many implementations of edge detection algorithms, the signal is convolved with the <strong>Laplacian of a Gaussian</strong> (LoG) $g_{\sigma}^{‘’}$:</p><p>$$s^{‘’}(x) = g_{\sigma}^{‘’} * I(x)$$</p><p><img src="zero_crossings.png"></p><p>In multi-scale edge detection, using a small $\sigma$ brings out all the edges. As $\sigma$ increases, the signal is smoothed more and more and only the central edge survives. The amount of smoothing controls the scale at which we analyse the image. Fine scale edge detection is particularly sensitive to noise.</p><h3 id="2D-edge-detection"><a href="#2D-edge-detection" class="headerlink" title="2D edge detection"></a>2D edge detection</h3><p>The 1D edge detection scheme can be extended to work in two dimensions. First we smooth the image $I(x, y)$ by convolving with a 2D Gaussian $G_{\sigma}(x, y)$:</p><p>$$G_{\sigma}(x, y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2+y^2}{2\sigma^2}}$$ </p><p>$$S(x, y) = G_{\sigma}(x, y) * I(x, y) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} G_{\sigma}(u, v)I(x-u, y-v)dudv$$</p><p>The next step is to find the gradient of the smoothed image $S(x, y)$ at every pixel:</p><p>$$\Delta S = \Delta (G_{\sigma} * I) = \begin{bmatrix} \frac{\partial (G_{\sigma} * I)}{\partial x} \\ \frac{\partial (G_{\sigma} * I)}{\partial y} \end{bmatrix} = \begin{bmatrix} \frac{\partial G_{\sigma}}{\partial x} * I \\ \frac{\partial G_{\sigma}}{\partial y} * I \end{bmatrix}$$</p><p>The next stage of the edge detection algorithm is non-maximal suppression. Edge elements, or edgels, are placed at locations where $|\Delta S|$ is greater than local values of $|\Delta S|$ in the directions $\pm \Delta S$. This aims to ensure that all edgels are located at ridge-points of the surface $|\Delta S|$. Subsequently, the edgels are thresholded, so that only those with $|\Delta S|$ above a certain value are retained.</p><p>An alternative approach to edge detection is to find zero-crossings of $\Delta^{2}G_{\sigma} * I$, where $\Delta^{2}G_{\sigma}$ is the Laplacian of $G_{\sigma}$.</p><p><img src="gaussian.png"></p><h3 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h3><p>In practice, the image and filter kernels are discrete quantities and the convolutions are performed as truncated summations:</p><p>$$S(x, y) = \sum_{u=-n}^{n} \sum_{u=-n}^{n} G_{\sigma}(u, v)I(x-u, y-v)$$</p><p><img src="kernel.png"></p><p>For acceptable accuracy, kernels are generally truncated so that the discarded samples are less than $1/1000$ of the peak value. The 2D convolutions would appear to be computationally expensive. However, they can be decomposed into two 1D convolutions:</p><p>$$G_{\sigma}(x, y) * I(x, y) = g_{\sigma}(x) * \left[g_{\sigma}(y) * I(x, y) \right]$$</p><p>Differentiation of the smoothed image is also implemented with a discrete convolution. By considering the Taylor-series expansion of $S(x, y)$ it is easy to show that a simple finite-difference approximation to the first-order spatial derivative of $S(x, y)$ is given by:</p><p>$$\frac{\partial S}{\partial x} = \frac{S(x+1, y) - S(x-1, y)}{2}$$</p><p>This is equivalent to convolving the rows of image samples, $S(x, y)$, with the kernel:</p><p>$$\begin{bmatrix} 1/2 &amp; 0 &amp; -1/2 \end{bmatrix}$$</p><h3 id="Corner-detection"><a href="#Corner-detection" class="headerlink" title="Corner detection"></a>Corner detection</h3><p>In an image, a corner is characterized by an intensity discontinuity in two directions. This discontinuity can be detected using <strong>correlation</strong>. The normalized <strong>cross-correlation</strong> function measures how well an image patch $P(x, y)$ matches other portions of the image, $I(x, y)$, as it is shifted from its original location. It entails sliding the patch over the image, computing the sum of the products of the pixels and normalizing the result:</p><p>$$c(x, y) = \frac{\sum_{u=-n}^{n} \sum_{u=-n}^{n} P(u, v)I(x+u, y+v)}{\sqrt{\sum_{u=-n}^{n} \sum_{u=-n}^{n} P^{2}(u, v) \sum_{u=-n}^{n} \sum_{u=-n}^{n} I^{2}(x+u, y+v)}}$$</p><p>A patch which has a well-defined peak in its correlation function can be classified as a “corner”.</p><p><img src="corner_detection.png"></p><p>A practical corner detection algorithm calculates change in intensity in direction $\mathbf{n}$ and smooths the result by convolution with a Gaussian kernel:</p><p>$$I_{n} \equiv \Delta I(x, y) \cdot \mathbf{n} \equiv \begin{bmatrix} I_{x} &amp; I_{y} \end{bmatrix}^{T} \cdot \mathbf{n} \equiv \begin{bmatrix} \frac{\partial I}{\partial x} &amp; \frac{\partial I}{\partial y} \end{bmatrix}^{T} \cdot \mathbf{n}$$</p><p>$$I_{n}^{2} = \frac{\mathbf{n}^{T} \Delta I \Delta I^{T} \mathbf{n}}{\mathbf{n}^{T} \mathbf{n}} = \frac{\mathbf{n}^{T} \begin{bmatrix} I_{x}^{2} &amp; I_{x}I_{y} \\ I_{x}I_{y} &amp; I_{y}^{2} \end{bmatrix} \mathbf{n}}{\mathbf{n}^{T} \mathbf{n}}$$</p><p>$$C_{n}(x, y) = G_{\sigma}(x, y) * I_{n}^{2} = \frac{\mathbf{n}^{T} \begin{bmatrix} \langle I_{x}^{2}\rangle &amp; \langle I_{x}I_{y}\rangle \\ \langle I_{x}I_{y}\rangle &amp; \langle I_{y}^{2}\rangle \end{bmatrix} \mathbf{n}}{\mathbf{n}^{T} \mathbf{n}} \equiv \frac{\mathbf{n}^{T} A \mathbf{n}}{\mathbf{n}^{T} \mathbf{n}}$$</p><p>Elementary eigenvector theory shows that $\lambda_{1} \leq C_{n}(x, y) \leq \lambda_{2}$ where $\lambda_{1}$ and $\lambda_{2}$ are the eigenvalues of $A$. Therefore, image structure around each pixel can be classified by looking at the eigenvalues:</p><ol><li>No structure (smooth variation): $\lambda_{1} \approx \lambda_{2} \approx 0$</li><li>1D structure (edge): $\lambda_{1} \approx 0$, $\lambda_{2}$ large (normal to edge)</li><li>2D structure (corner): $\lambda_{1}$ and $\lambda_{2}$ both large and distinct</li></ol><p>Corners are most useful for tracking in image sequences or matching in stereo pairs.</p><h3 id="Blob-detection"><a href="#Blob-detection" class="headerlink" title="Blob detection"></a>Blob detection</h3><p>A blob is an area of uniform intensity in the image. The minimum of the resulting response from the scale-normalised Laplacian of the Gaussian at the correct scale localises the centre of the dots:</p><p><img src="blob_detection.png"></p><p>The size of the blob detected depends on the sigma of the detector used. As the sigma is increased, larger and larger image features are detected, ranging from small boxes to entire buildings. The (scale-normalised) Laplacian of a Gaussian as recorded at a particular location is a smooth function over scale, with definite peaks or troughs. These maxima and minima occur at the centre of blobs whilst the scale defines its size.</p><p><img src="scale_space.png"></p><p>The <strong>Difference of Gaussians</strong> (DoG) is a blob detector. It is calculated as the difference of two Gaussians, which approximates the scalenormalised Laplacian of a Gaussian.</p><p>$$G(x, y, k\sigma) - G(x, y, \sigma) \approx (k - 1)\sigma^{2} \Delta^{2} G(x, y, \sigma)$$</p><h2 id="Kernels-used-for-image-processing-algorithms"><a href="#Kernels-used-for-image-processing-algorithms" class="headerlink" title="Kernels used for image processing algorithms"></a>Kernels used for image processing algorithms</h2><p>Image Processing Visuailizer features a collection of commonly used kernels for image processing and feature extraction. In practice, the kernels are convoluted across the image to produce the desired output.</p><p><img src="algorithms.jpg"></p><h3 id="Box-blur"><a href="#Box-blur" class="headerlink" title="Box blur"></a>Box blur</h3><p>A box blur (also known as a box linear filter) is a spatial domain linear filter in which each pixel in the resulting image has a value equal to the average value of its neighbouring pixels in the input image. It is a form of low-pass (“blurring”) filter. </p><p>$$\mathbf{G} = \frac{1}{9} \begin{bmatrix}1 &amp; 1 &amp; 1\\1 &amp; 1 &amp; 1\\1 &amp; 1 &amp; 1\end{bmatrix} * \mathbf{A}$$ </p><p>Box blurs are frequently used to approximate a Gaussian blur. By the central limit theorem, repeated application of a box blur will approximate a Gaussian blur.</p><h3 id="Gaussian-blur"><a href="#Gaussian-blur" class="headerlink" title="Gaussian blur"></a>Gaussian blur</h3><p>In image processing, a Gaussian blur (also known as Gaussian smoothing) is the result of blurring an image by a Gaussian function. Mathematically, applying a Gaussian blur to an image is the same as convolving the image with a Gaussian function. This is also known as a two-dimensional Weierstrass transform. </p><p>$$G(x,y) = \frac{1}{2\pi\sigma^2} e^{-\frac{x^2+y^2}{2\sigma^2}}$$ </p><p>Gaussian smoothing is also used as a pre-processing stage in computer vision algorithms in order to enhance image structures at different scales.</p><h3 id="Sobel-operator"><a href="#Sobel-operator" class="headerlink" title="Sobel operator"></a>Sobel operator</h3><p>The operator uses two 3×3 kernels which are convolved with the original image to calculate approximations of the derivatives – one for horizontal changes, and one for vertical. </p><p>$$\mathbf{G}_x = \begin{bmatrix}+1 &amp; 0 &amp; -1\\+2 &amp; 0 &amp; -2\\+1 &amp; 0 &amp; -1\end{bmatrix} * \mathbf{A}$$<br>$$\mathbf{G}_y = \begin{bmatrix}+1 &amp; +2 &amp; +1\\0 &amp; 0 &amp; 0\\-1 &amp; -2 &amp; -1\end{bmatrix} * \mathbf{A}$$</p><p>The x-coordinate is defined here as increasing in the “right”-direction, and the y-coordinate is defined as increasing in the “down”-direction. At each point in the image, the resulting gradient approximations can be combined to give the gradient magnitude and direction, using: </p><p>$$\mathbf{G} = \sqrt{\mathbf{G}_x^2 + \mathbf{G}_y^2}$$<br>$$\mathbf{\Theta} = \arctan{\frac{\mathbf{G}_y}{\mathbf{G}_x}}$$</p><h3 id="Prewitt-operator"><a href="#Prewitt-operator" class="headerlink" title="Prewitt operator"></a>Prewitt operator</h3><p>The operator uses two 3×3 kernels which are convolved with the original image to calculate approximations of the derivatives – one for horizontal changes, and one for vertical. </p><p>$$\mathbf{G}_x = \begin{bmatrix}+1 &amp; 0 &amp; -1\\+1 &amp; 0 &amp; -1\\+1 &amp; 0 &amp; -1\end{bmatrix} * \mathbf{A}$$<br>$$\mathbf{G}_y = \begin{bmatrix}+1 &amp; +1 &amp; +1\\0 &amp; 0 &amp; 0\\-1 &amp; -1 &amp; -1\end{bmatrix} * \mathbf{A}$$ </p><h3 id="Canny-edge-detector"><a href="#Canny-edge-detector" class="headerlink" title="Canny edge detector"></a>Canny edge detector</h3><p>The algorithm takes the output of a Sobel operator and categorizes the continuous gradient directions into a small set of discrete directions, and then moves a 3x3 filter over the output of the previous step (that is, the edge strength and gradient directions). At every pixel, it suppresses the edge strength of the centre pixel (by setting its value to 0) if its magnitude is not greater than the magnitude of the two neighbours in the gradient direction.</p><h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><p>Code for this project can be found in <a href="https://github.com/ziyi-zhu/image-processing-visualizer">GitHub repository</a>.</p><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><p>Install Express in the <code>root</code> directory and save it in the dependencies list. For example:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install express --save<br></code></pre></td></tr></table></figure><p>To install Express temporarily and not add it to the dependencies list:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install express --no-save<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Node.js</tag>
      
      <tag>Javascript</tag>
      
      <tag>Computer vision</tag>
      
      <tag>Computer graphics</tag>
      
      <tag>Image processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Autonomous Robot Navigation Using Computer Vision</title>
    <link href="/2020/03/02/cv-robot-navigation/"/>
    <url>/2020/03/02/cv-robot-navigation/</url>
    
    <content type="html"><![CDATA[<p>Autonomous navigation is widely used in robotics or any other mobile device as the ability to navigate in their environment is crucial for safe and successful operations. Computer vision is one of the most popular methods in autonomous navigation as the algorithms can extract visual features and analyze complex situations in a variety of environment. This project aims to explore different algorithmic approaches to autonomous navigation and design a self-navigating robot for rescue tasks in a tunnel-like environment.</p><span id="more"></span><p><img src="output.gif"></p><h2 id="Introduction-to-robot-navigation"><a href="#Introduction-to-robot-navigation" class="headerlink" title="Introduction to robot navigation"></a>Introduction to robot navigation</h2><p>Navigation can be defined as the combination of the three fundamental competencies:</p><ol><li>Self-localisation</li><li>Path planning</li><li>Map-building and map interpretation</li></ol><p>Robot localization denotes the robot’s ability to establish its position and orientation within the frame of reference. Path planning is effectively an extension of localisation, in that it requires the determination of the robot’s current position and a position of a goal location, both within the same frame of reference or coordinates. Map building can be in the shape of a metric map or any notation describing locations in the robot frame of reference.</p><h2 id="Basic-colour-detection-with-OpenCV"><a href="#Basic-colour-detection-with-OpenCV" class="headerlink" title="Basic colour detection with OpenCV"></a>Basic colour detection with OpenCV</h2><p>Vision-based navigation or optical navigation uses computer vision algorithms and optical sensors to extract the visual features required to the localization in the surrounding environment. The general idea of colour detection is to apply a threshold mask to the image and remove all the pixels that are outside the range of the specified colour. Moreover, it is easier to work in HSV colour space as normally we would like to select pixels with a certain hue but with less constraint on their saturation and value.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Threshold of green in HSV space </span><br>lower = np.array([<span class="hljs-number">40</span>, <span class="hljs-number">40</span>, <span class="hljs-number">40</span>]) <br>upper = np.array([<span class="hljs-number">60</span>, <span class="hljs-number">255</span>, <span class="hljs-number">255</span>])<br><br><span class="hljs-comment"># Convert the BGR color space of image to HSV color space </span><br>hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) <br><br><span class="hljs-comment"># Mask out any non-essential part of the frame</span><br>hsv = cv2.bitwise_and(hsv, hsv, mask=mask)<br><br><span class="hljs-comment"># Prepare the mask to overlay and extract pixels within the threshold</span><br>thresh = cv2.inRange(hsv, lower, upper)<br></code></pre></td></tr></table></figure><p>After the coloured pixels are selected, there may be some noise in the image since the result of the colour detection algorithm depends strongly on the lighting condition as well as camera settings. Therefore, a series of actions is needed to filter out the noise and make the algorithm more robust against environmental changes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Perform a series of erosions and dilations to remove</span><br><span class="hljs-comment"># any small blobs of noise from the thresholded image</span><br>thresh = cv2.erode(thresh, <span class="hljs-literal">None</span>, iterations=<span class="hljs-number">1</span>)<br>thresh = cv2.dilate(thresh, <span class="hljs-literal">None</span>, iterations=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>After the coloured pixels are extracted from the original frame, we need an algorithm to determine the location of each blob of pixels. Different methods of blob detection can be used including the connected-component analysis, which is an algorithmic application of graph theory, where subsets of connected components are uniquely labelled based on a given heuristic. Alternatively, OpenCV has a built-in simple blob detector which works well for quick and accurate detection.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Set up the detector with custom parameters</span><br>params = cv2.SimpleBlobDetector_Params()<br>params.filterByArea = <span class="hljs-literal">True</span><br>params.minArea = <span class="hljs-number">30</span><br>params.maxArea = <span class="hljs-number">1000</span><br>params.filterByCircularity = <span class="hljs-literal">False</span><br>params.filterByColor = <span class="hljs-literal">False</span><br>params.filterByConvexity = <span class="hljs-literal">False</span><br>params.filterByInertia = <span class="hljs-literal">False</span><br><br>detector = cv2.SimpleBlobDetector_create(params)<br><br><span class="hljs-comment"># Detect blobs and save keypoints as np array</span><br>kps = detector.detect(thresh)<br></code></pre></td></tr></table></figure><p>Note that the <code>minArea</code> and <code>maxArea</code> of the blobs need to be specified for the detector to function properly. The area of the blobs may depend on the resolution of the image. Now that the locations of each blob are obtained, we need to work out the location and orientation of the robot based on the information we get from these key points.</p><p><img src="detection.jpg"></p><h2 id="Algorithmic-approach-to-self-localisation"><a href="#Algorithmic-approach-to-self-localisation" class="headerlink" title="Algorithmic approach to self-localisation"></a>Algorithmic approach to self-localisation</h2><p>In linear algebra, a rotation matrix is a matrix that is used to perform a rotation in Euclidean space.</p><p>$$\boldsymbol{R} = \begin{bmatrix} \cos{\theta} &amp; -\sin{\theta} \\ \sin{\theta} &amp; cos{\theta} \\ \end{bmatrix}$$</p><p>The approach of localising the robot is to paste a label on top of it with magenta squares at three corners of the paper. Once we retrieve the keypoints from the image, we can work out the edges and the diagonal of the label and with simple sorting and comparison, the centre position vector and top left corner of the label can be obtained and stored in variables. A 45-degree rotation matrix then allows the orientation of the robot to be computed by rotating the vector pointing from the centre position to the top left corner by 45 degrees.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Create a 45 degree rotation matrix</span><br>theta = np.radians(<span class="hljs-number">45</span>)<br>c, s = np.cos(theta), np.sin(theta)<br>R = np.array(((c,-s), (s, c)))<br><br><span class="hljs-comment"># Rotate the diagonal vector</span><br>dirn = R.dot(top_left - pos)<br></code></pre></td></tr></table></figure><p>In practice, a QR code can also be pasted on top of the robot in order to find the location and orientation of the robot with the ZBar module in python. However, this requires cameras with good quality and resolution to consistently detect the QR code.</p><p><img src="localisation.jpg"></p><h2 id="Robot-control-with-Arduino"><a href="#Robot-control-with-Arduino" class="headerlink" title="Robot control with Arduino"></a>Robot control with Arduino</h2><p>The software system consists of two main programs:</p><ol><li>C++ program is uploaded to the Arduino Uno Wifi Rev 2 with Adafruit<br>Motor Shield for servo and motor control.</li><li>Computer vision program is written in python for robot navigation and sequence control.</li></ol><p>For the computer vision program, OpenCV is used for real-time image processing and object detection. The communication between Arduino and workstation is realized using the built-in Wifi function and through HTTP requests. Arduino receives and analyses the instruction and controls the robot movement according to the angle and distance from the calculated target point. Subsequently, functions for grabbing and dropping victims are called when the respective target points are reached.</p><p><img src="side.jpg"></p><h3 id="Pinout"><a href="#Pinout" class="headerlink" title="Pinout"></a>Pinout</h3><p>Arduino Wifi Rev 2 is used with the Adafruit motor shield for motor and servo control:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++">Adafruit_MotorShield AFMS = <span class="hljs-built_in">Adafruit_MotorShield</span>(); <br></code></pre></td></tr></table></figure><p>The pintout for Arduino can ba changed in <code>arduino/Bin.ino</code>:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> led_abr = <span class="hljs-number">2</span>;<br>myservo.<span class="hljs-built_in">attach</span>(<span class="hljs-number">9</span>);       <span class="hljs-comment">// attaches the servo on pin 9 to the servo object</span><br></code></pre></td></tr></table></figure><p>Servo 2 on the Adafruit shield corresponds to pin 9 on the Arduino.</p><h3 id="Wifi"><a href="#Wifi" class="headerlink" title="Wifi"></a>Wifi</h3><p>The network SSID name and network password need to be specified in <code>arduino/Bin.ino</code> for HTTP requests:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">char</span> ssid[] = SECRET_SSID;        <span class="hljs-comment">// your network SSID (name)</span><br><span class="hljs-keyword">char</span> pass[] = SECRET_PASS;        <span class="hljs-comment">// your network password (use for WPA, or use as key for WEP)</span><br><span class="hljs-keyword">int</span> keyIndex = <span class="hljs-number">0</span>;                 <span class="hljs-comment">// your network key Index number (needed only for WEP)</span><br></code></pre></td></tr></table></figure><h2 id="Robot-design-and-hardware"><a href="#Robot-design-and-hardware" class="headerlink" title="Robot design and hardware"></a>Robot design and hardware</h2><p>The claw mechanism consists of an electric motor for opening and closing of the claw and a servo for the upward and downward motion. An Infrared receiver is used for victim health detection and two Optoswitches are used for line-following. </p><p>Our initial plan was to detect the distance of the robot from victims using an ultrasonic sensor. However, since the computer vision algorithm can also detect distances to a desirable accuracy, we decided not to use the sensor. After experimenting with the line-following circuit, we decided to abandon this part of the program as well and rely solely on computer vision for all robot controls and movement. This results in a simple and elegant code structure but causes problems in the end due to the malfunction of the camera in the actual competition.</p><p><img src="front.jpg"></p><h2 id="Challenges-and-next-steps"><a href="#Challenges-and-next-steps" class="headerlink" title="Challenges and next steps"></a>Challenges and next steps</h2><p>Our robot had a poor performance during the competition due to the malfunction of the camera as the exposure is too high for the robot to be accurately<br>located. We suspect the reason to be the shadows cast by people surrounding the table causing the camera to increase the exposure, resulting in abnormally<br>desaturated colour at the centre of the table which is exposed to direct lighting from the ceiling. This is not a difficult problem to address as the tolerance for colour detection can be easily adjusted. However, since it was the first time we ever encountered this problem, we did not manage to solve the issue on the spot and the time eventually ran out.</p><p>In conclusion, autonomous robot navigation using computer vision remains a very challenging problem and requires more robust algorithms with testing in a variety of conditions to validate the safety and reliability of the system. Throughout the course of this project, we made a valid attempt to tackle these issues and achieved applaudable results in the end despite some imperfections in our system. It is important to be open-minded and consider factors that might be affecting these computer vision systems in a different environment so that they can survive in the real world where, arguably, nothing is predictable.</p><h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><p>Code for this project can be found in <a href="https://github.com/ziyi-zhu/cv_robot_navigation">GitHub repository</a>.</p><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><p>Install requests from PyPI:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">sudo pip install requests<br></code></pre></td></tr></table></figure><p>Install OpenCV from GitHub source.</p><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p>Video demo of the robot can be found on <a href="https://youtu.be/kYkNHKw41CQ">Youtube channel</a>.</p>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Computer vision</tag>
      
      <tag>OpenCV</tag>
      
      <tag>IoT</tag>
      
      <tag>Control</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Waste Collection and Management Using IoT and Machine Learning</title>
    <link href="/2020/01/25/smart-waste/"/>
    <url>/2020/01/25/smart-waste/</url>
    
    <content type="html"><![CDATA[<p>Having gone through projects available for Hack Cambridge 101, we are inspired by the Smart Waste challenge put forwarded by Reply. This is because current waste collection system is rather inefficient and a lot of resources has been wasted during the recycling process. Therefore, this project aims to address such urgent need for improvement in this sector.</p><span id="more"></span><p><img src="bin.jpg"></p><p>Our system has a number of key functions:</p><ul><li>Classifies incoming waste into categories (Recyclable/Organic)</li><li>Detects whether the bin is full</li><li>Optimizes waste collection routes</li></ul><h2 id="Hardware-design"><a href="#Hardware-design" class="headerlink" title="Hardware design"></a>Hardware design</h2><p>For the hardware, we started by building a bin using a cardboard box. Then we attached an Arduino board to the box which acts as the main control system. A servo motor, an ultrasonic sensor, a camera and a LED were also installed.</p><h3 id="Arduino-wiring"><a href="#Arduino-wiring" class="headerlink" title="Arduino wiring"></a>Arduino wiring</h3><p><img src="circuit.jpg"></p><h3 id="Pinout"><a href="#Pinout" class="headerlink" title="Pinout"></a>Pinout</h3><p>The pintout for Arduino can ba changed in <code>arduino/Bin.ino</code>:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> trigPin = <span class="hljs-number">9</span>;<br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> echoPin = <span class="hljs-number">10</span>;<br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> ledPin = <span class="hljs-number">13</span>;<br><span class="hljs-keyword">const</span> <span class="hljs-keyword">int</span> buttonPin = A2;<br></code></pre></td></tr></table></figure><h3 id="Camera"><a href="#Camera" class="headerlink" title="Camera"></a>Camera</h3><p>Spy camera is used for Computer Vision.</p><p><img src="camera.jpg"></p><h3 id="Sensor"><a href="#Sensor" class="headerlink" title="Sensor"></a>Sensor</h3><p>An ultrasonic sensor is used to detect the capacity of the bin.</p><p><img src="sensor.jpg"></p><h2 id="Software-control-and-image-recognition"><a href="#Software-control-and-image-recognition" class="headerlink" title="Software control and image recognition"></a>Software control and image recognition</h2><p>In terms of the software, Arduino code is used to control all components. A Keras model detects whether the object is recyclable. A python API was also built to calculate the optimized route and plot out the route on Google map.</p><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><p>Install Keras from PyPI:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">sudo pip install Keras<br></code></pre></td></tr></table></figure><p>Install TensorFlow and OpenCV from GitHub source.</p><h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p>Connect the machine to Arduino and run <code>python/predict.py</code>.</p><h3 id="Recycling-with-computer-vision"><a href="#Recycling-with-computer-vision" class="headerlink" title="Recycling with computer vision"></a>Recycling with computer vision</h3><p>Recyclable wastes will be dropped inside the bin.</p><p><img src="bag.gif"><br><img src="can.gif"></p><p>Non-recyclable wastes will be dropped outside the bin.</p><p><img src="banana.gif"><br><img src="orange.gif"></p><h3 id="Bin-capacity-detection"><a href="#Bin-capacity-detection" class="headerlink" title="Bin capacity detection"></a>Bin capacity detection</h3><p>If the bin is full, the green LED will light up.</p><p><img src="full.gif"></p><h3 id="Route-planning-with-OptimoRoute-and-Google-Map-Services"><a href="#Route-planning-with-OptimoRoute-and-Google-Map-Services" class="headerlink" title="Route planning with OptimoRoute and Google Map Services"></a>Route planning with OptimoRoute and Google Map Services</h3><p>When the bin is full, its address will be uploaded and order for waste pick up will be submitted to the OptimoRoute website. The best route can be obtained from API and plotted using Google Map.</p><p><img src="route.png"></p><h2 id="Lessons-learned"><a href="#Lessons-learned" class="headerlink" title="Lessons learned"></a>Lessons learned</h2><p>The biggest challenge we met was due to the poor quality of the camera. We initially trained the ML model using high-quality photo datasets from Imagenet. However, this didn’t work well initially as the photos collected using the spy camera were of low quality, contrast and saturation. We recognized the problem and manually collected another dataset using the spy camera. In the end, the accuracy reached around 90%. Higher accuracy can be achieved using a better camera.</p><h2 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next"></a>What’s next</h2><p>The core of the project is the ML model. We only had time to collect around 100 photos using the spy camera, if we had more time we could have collected more photos so that we have a larger training set. We expect this to improve its classification accuracy. The concept and the technology developed can be implemented on a large scale in urban regions, where we envision IoT enabled bins to be used in parks, public areas and homes. These devices will enable more efficient waste collection methods to be used by the council and are expected to greatly reduce the resources needed to collect public wastes. </p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IoT</tag>
      
      <tag>TensorFlow</tag>
      
      <tag>Machine learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hardware and Software for Capturing HDR Light Fields</title>
    <link href="/2019/09/02/hdr-lf-capture/"/>
    <url>/2019/09/02/hdr-lf-capture/</url>
    
    <content type="html"><![CDATA[<p>This project aims to design a system that captures HDR light fields to generate image-based 3D reconstruction for display on the HDR multi-focal stereoscope. The setup consists of an Arduino used to control the Stepper Motor and a Raspberry Pi used to send instructions to the Arduino and capture/receive images from the camera.</p><span id="more"></span><p><img src="header.jpg"></p><h2 id="Hardware-design"><a href="#Hardware-design" class="headerlink" title="Hardware design"></a>Hardware design</h2><h3 id="Arduino-wiring"><a href="#Arduino-wiring" class="headerlink" title="Arduino wiring"></a>Arduino wiring</h3><p><img src="arduino.jpg"></p><p>The Stepper Driver used in this setup is DQ542MA and the Stepper Motor used is NEMA 17 42BYGH-W811.</p><p>Connection between Stepper Driver and Arduino:</p><ul><li>PUL+ from Driver to Arduino PIN7</li><li>PUL- from Driver to Arduino PIN5</li><li>DIR+ from Driver to Arduino PIN7</li><li>DIR- from Driver to Arduino PIN6</li><li>ENBL+ from Driver to Arduino PIN7</li></ul><p>Connection between Stepper Driver and Stepper Motor:</p><ul><li>A+ from Driver to Stepper BLACK wire</li><li>A- from Driver to Stepper GREEN wire</li><li>B+ from Driver to Stepper RED wire</li><li>B- from Driver to Stepper BLUE wire</li></ul><p>Connection between Button and Arduino:</p><ul><li>Button YELLOW wire to Arduino PINA0</li></ul><h3 id="DIP-switch-setting"><a href="#DIP-switch-setting" class="headerlink" title="DIP switch setting"></a>DIP switch setting</h3><p><img src="switch.jpg"></p><p>The first three bits of the DIP switch are used to set the dynamic current. For an output current of 2.37A, SW 1, 2, 3 is set to OFF, OFF, ON.</p><p>Micro step resolution is set by the last four bits of the DIP switch. For 1600 pulse per revolution, SW 5, 6, 7, 8 is set to OFF, OFF, ON, ON.</p><h3 id="Power-supply"><a href="#Power-supply" class="headerlink" title="Power supply"></a>Power supply</h3><p><img src="battery.jpg"></p><p>The power supply for the Stepper Driver is 18V (3 parallel sets of 2 9V batteries in series).</p><h2 id="Serial-communication-with-Python"><a href="#Serial-communication-with-Python" class="headerlink" title="Serial communication with Python"></a>Serial communication with Python</h2><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><p>Installing libgphoto2:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">sudo apt-get install libgphoto2-dev<br></code></pre></td></tr></table></figure><p>Installing python-gphoto2 and pySerial for python3 with pip:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">sudo pip3 install gphoto2<br>sudo pip3 install pyserial<br></code></pre></td></tr></table></figure><h3 id="Usage-Examples"><a href="#Usage-Examples" class="headerlink" title="Usage Examples"></a>Usage Examples</h3><p>Specify the port and baud rate for the serial communication between Arduino and Raspberry Pi in <code>python/camera.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">port = <span class="hljs-string">&#x27;/dev/ttyACM0&#x27;</span><br>baud_rate = <span class="hljs-number">9600</span><br></code></pre></td></tr></table></figure><p>Set the parameters for HDR Light Field capture in <code>python/gui.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">camera_name = <span class="hljs-string">&#x27;SonyA7r1&#x27;</span><br><span class="hljs-comment"># hdr_merging = False</span><br></code></pre></td></tr></table></figure><p>(Optional) Merge the captured images into HDR light field (requires OpenCV):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> merge <span class="hljs-keyword">import</span> merge_light_field<br>merge_light_field(capture_path, camera_name, n_exposures)<br></code></pre></td></tr></table></figure><h2 id="GUI-design-with-python"><a href="#GUI-design-with-python" class="headerlink" title="GUI design with python"></a>GUI design with python</h2><p><img src="gui.jpg"></p><p>GUI allows for basic motor and camera control:</p><ul><li>Motor speed can be set from 1 to 10. (disabled)</li><li>Camera location can range from 0 to 1000.</li><li>Number of exposures and stops can be set according to camera settings.</li><li>Number of views can range from 1 to 1001.</li><li>Click <code>Capture</code> to capture a single image.</li><li>Click <code>Capture LF</code> to capture HDR light field.</li></ul><p>Uncomment relevant code in <code>python/gui.py</code> to enable different controls.</p><h2 id="Release-History"><a href="#Release-History" class="headerlink" title="Release History"></a>Release History</h2><ul><li>0.2.0<ul><li>Edited code structure and added more controls</li><li>CHANGE: Seperate <code>control.py</code> into and <code>control.py</code> and <code>gui.py</code></li></ul></li><li>0.1.0<ul><li>The first proper release</li><li>CHANGE: Add <code>arduino</code> and <code>python</code></li></ul></li><li>0.0.1<ul><li>Work in progress</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>Computer Vision</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IoT</tag>
      
      <tag>Computer graphics</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
